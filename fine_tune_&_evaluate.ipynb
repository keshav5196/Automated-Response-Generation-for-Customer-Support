{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Kaludi/Customer-Support-Responses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate query and response for fine-tuning\n",
    "def concatenate_examples(examples):\n",
    "    return {'text': [q + \" \" + r for q, r in zip(examples['query'], examples['response'])]}\n",
    "\n",
    "# Apply the concatenation function to the dataset\n",
    "dataset = dataset.map(concatenate_examples, batched=True, remove_columns=[\"query\", \"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=0.01\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine-tuned-gpt2-tokenizer/tokenizer_config.json',\n",
       " './fine-tuned-gpt2-tokenizer/special_tokens_map.json',\n",
       " './fine-tuned-gpt2-tokenizer/vocab.json',\n",
       " './fine-tuned-gpt2-tokenizer/merges.txt',\n",
       " './fine-tuned-gpt2-tokenizer/added_tokens.json')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the model\n",
    "trainer.save_model(\"./fine-tuned-gpt2\")\n",
    "tokenizer.save_pretrained(\"./fine-tuned-gpt2-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./fine-tuned-gpt2\")\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./fine-tuned-gpt2-tokenizer\")\n",
    "# tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: My order hasn't arrived yet.\n",
      "Reference Response: We apologize for the inconvenience. Can you please provide your order number?\n",
      "Generated Response: My order hasn't arrived yet. I'm not sure if it's because I'm not sure what to do with it, or if it's just a little too much for me. I'm not sure if it's because I'm not sure what\n",
      "Perplexity: 4.518495559692383\n",
      "BLEU Score: 8.669612184277444e-232\n",
      "ROUGE Score: {'rouge1': Score(precision=0.0425531914893617, recall=0.16666666666666666, fmeasure=0.06779661016949153), 'rougeL': Score(precision=0.02127659574468085, recall=0.08333333333333333, fmeasure=0.03389830508474576)}\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: I need to return an item.\n",
      "Reference Response: Certainly. Please provide your order number and reason for return, and we will provide you with instructions on how to proceed.\n",
      "Generated Response: I need to return an item.\n",
      "\n",
      "If you have a problem with the item, please contact us.\n",
      "\n",
      "If you have a problem with the item, please contact us.\n",
      "\n",
      "If you have a problem with the item, please contact\n",
      "Perplexity: 3.548099994659424\n",
      "BLEU Score: 9.65701126654974e-232\n",
      "ROUGE Score: {'rouge1': Score(precision=0.13157894736842105, recall=0.23809523809523808, fmeasure=0.1694915254237288), 'rougeL': Score(precision=0.07894736842105263, recall=0.14285714285714285, fmeasure=0.1016949152542373)}\n",
      "--------------------------------------------------\n",
      "Query: I want to change my shipping address.\n",
      "Reference Response: No problem. Can you please provide your order number and the new shipping address you'd like to use?\n",
      "Generated Response: I want to change my shipping address. I want to change my shipping address.\n",
      "\n",
      "I want to change my shipping address. I want to change my shipping address.\n",
      "\n",
      "I want to change my shipping address. I want to change my shipping\n",
      "Perplexity: 2.2847697734832764\n",
      "BLEU Score: 8.561894227089738e-232\n",
      "ROUGE Score: {'rouge1': Score(precision=0.07317073170731707, recall=0.15789473684210525, fmeasure=0.09999999999999999), 'rougeL': Score(precision=0.07317073170731707, recall=0.15789473684210525, fmeasure=0.09999999999999999)}\n",
      "--------------------------------------------------\n",
      "Average BLEU Score: 8.962839225972308e-232\n"
     ]
    }
   ],
   "source": [
    "# Load the evaluation dataset\n",
    "eval_data = [\n",
    "    {\"query\": \"My order hasn't arrived yet.\", \"response\": \"We apologize for the inconvenience. Can you please provide your order number?\"},\n",
    "    {\"query\":\"I need to return an item.\",\"response\":\"Certainly. Please provide your order number and reason for return, and we will provide you with instructions on how to proceed.\"},\n",
    "    {\"query\":\"I want to change my shipping address.\",\"response\":\"No problem. Can you please provide your order number and the new shipping address you'd like to use?\"},  \n",
    "]\n",
    "\n",
    "# Function to generate responses\n",
    "def generate_response(query):\n",
    "    inputs = tokenizer.encode(query, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs, max_length=50, num_return_sequences=1)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Calculate Perplexity\n",
    "def calculate_perplexity(model, tokenizer, text):\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "    return torch.exp(loss)\n",
    "\n",
    "# Initialize metrics\n",
    "bleu_scores = []\n",
    "rouge_scorer_1 = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Evaluate the model\n",
    "for example in eval_data:\n",
    "    query = example[\"query\"]\n",
    "    reference_response = example[\"response\"]\n",
    "    generated_response = generate_response(query)\n",
    "    \n",
    "    # Calculate Perplexity\n",
    "    perplexity = calculate_perplexity(model, tokenizer, generated_response)\n",
    "    \n",
    "    # Calculate BLEU Score\n",
    "    reference_tokens = reference_response.split()\n",
    "    generated_tokens = generated_response.split()\n",
    "    bleu_score = sentence_bleu([reference_tokens], generated_tokens)\n",
    "    bleu_scores.append(bleu_score)\n",
    "    \n",
    "    # Calculate ROUGE Score\n",
    "    rouge_score = rouge_scorer_1.score(reference_response, generated_response)\n",
    "    \n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Reference Response: {reference_response}\")\n",
    "    print(f\"Generated Response: {generated_response}\")\n",
    "    print(f\"Perplexity: {perplexity}\")\n",
    "    print(f\"BLEU Score: {bleu_score}\")\n",
    "    print(f\"ROUGE Score: {rouge_score}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Calculate average BLEU Score\n",
    "average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "print(f\"Average BLEU Score: {average_bleu}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u>Observations:</u>**\n",
    "\n",
    "1. Based on the metrics on few data points, it can be seen that GPT-2 is not peforming well.\n",
    "\n",
    "**<u>Next steps to improve the performance:</u>**\n",
    "\n",
    "1. Increase dataset size either by generating synthetic data or getting more data.\n",
    "\n",
    "2. Using a bigger model or different model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
